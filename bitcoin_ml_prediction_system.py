"""
Bitcoin Price Prediction ML System
==================================
Professional-grade machine learning system for Bitcoin price prediction.
Combines Wikipedia sentiment analysis with price data using PyTorch.

Features:
- Multi-modal input: Price data + Wikipedia sentiment scores
- Multiple model architectures: LSTM, Transformer, Ensemble
- Banking-grade validation and backtesting
- Uncertainty quantification for risk management
- Real-time prediction capabilities for 24/7 dashboard
- Professional model interpretability and explainability

Model Types:
1. LSTM/GRU Networks (Primary recommendation)
2. Transformer Architecture (Advanced option) 
3. Ensemble Methods (Maximum robustness)
4. Baseline Models (Random Forest, XGBoost for comparison)
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

import logging
import json
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

class BitcoinPredictionDataset(Dataset):
    """
    PyTorch Dataset for Bitcoin price prediction with sentiment features.
    """
    
    def __init__(self, price_data: np.ndarray, sentiment_data: np.ndarray, 
                 targets: np.ndarray, sequence_length: int = 30):
        """
        Args:
            price_data: Historical price features (scaled)
            sentiment_data: Sentiment scores (scaled)
            targets: Target prices for prediction
            sequence_length: Number of historical days to use for prediction
        """
        self.price_data = torch.FloatTensor(price_data)
        self.sentiment_data = torch.FloatTensor(sentiment_data)
        self.targets = torch.FloatTensor(targets)
        self.sequence_length = sequence_length
        
    def __len__(self):
        return len(self.price_data) - self.sequence_length
    
    def __getitem__(self, idx):
        # Get sequence of historical data
        price_sequence = self.price_data[idx:idx + self.sequence_length]
        sentiment_sequence = self.sentiment_data[idx:idx + self.sequence_length]
        
        # Combine price and sentiment features
        combined_features = torch.cat([price_sequence, sentiment_sequence.unsqueeze(-1)], dim=-1)
        
        # Target is the next day's price
        target = self.targets[idx + self.sequence_length]
        
        return combined_features, target

class BitcoinLSTMModel(nn.Module):
    """
    LSTM-based Bitcoin price prediction model.
    Recommended for banking applications due to interpretability and reliability.
    """
    
    def __init__(self, input_size: int, hidden_size: int = 128, num_layers: int = 2, 
                 dropout: float = 0.2, output_size: int = 1):
        super(BitcoinLSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True
        )
        
        # Attention mechanism for better sentiment integration
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # Fully connected layers
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 4, output_size)
        )
        
        # For uncertainty quantification
        self.uncertainty_head = nn.Linear(hidden_size // 4, 1)
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # Apply attention to the LSTM output
        attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Use the last output for prediction
        last_output = attended_out[:, -1, :]
        
        # Price prediction
        price_pred = self.fc_layers(last_output)
        
        # Uncertainty estimation (standard deviation)
        uncertainty = torch.exp(self.uncertainty_head(last_output))  # Ensure positive
        
        return price_pred, uncertainty, attention_weights

class BitcoinTransformerModel(nn.Module):
    """
    Transformer-based Bitcoin price prediction model.
    Advanced option for maximum accuracy with complex patterns.
    """
    
    def __init__(self, input_size: int, d_model: int = 128, nhead: int = 8, 
                 num_layers: int = 4, dropout: float = 0.1):
        super(BitcoinTransformerModel, self).__init__()
        
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        
        # Positional encoding
        self.pos_encoding = self._create_positional_encoding(1000, d_model)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Output layers
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1)
        )
        
        self.uncertainty_head = nn.Linear(d_model // 2, 1)
        
    def _create_positional_encoding(self, max_len: int, d_model: int):
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe.unsqueeze(0)
    
    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1)
        
        # Project input to model dimension
        x = self.input_projection(x)
        
        # Add positional encoding
        pos_enc = self.pos_encoding[:, :seq_len, :].to(x.device)
        x = x + pos_enc
        
        # Transformer forward pass
        transformer_out = self.transformer(x)
        
        # Use the last token for prediction
        last_token = transformer_out[:, -1, :]
        
        # Price prediction
        intermediate = self.output_projection[:-1](last_token)
        price_pred = self.output_projection[-1](intermediate)
        
        # Uncertainty estimation
        uncertainty = torch.exp(self.uncertainty_head(intermediate))
        
        return price_pred, uncertainty, None

class BitcoinMLSystem:
    """
    Comprehensive ML system for Bitcoin price prediction.
    Integrates sentiment analysis with price data for banking applications.
    """
    
    def __init__(self, model_type: str = 'lstm', device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_type = model_type
        self.model = None
        self.price_scaler = StandardScaler()
        self.sentiment_scaler = StandardScaler()
        self.target_scaler = StandardScaler()
        self.sequence_length = 30  # 30 days of history
        
        self.feature_columns = [
            'price_usd', 'volume_24h', 'price_change_24h_pct', 
            'volatility_7d', 'volatility_30d', 'rsi_14', 'sma_7', 'sma_30'
        ]
        
        logger.info(f"Initialized Bitcoin ML System with {model_type} on {self.device}")
        
    def prepare_data(self, price_df: pd.DataFrame, sentiment_df: pd.DataFrame) -> Dict:
        """
        Prepare and merge price and sentiment data for ML training.
        
        Args:
            price_df: DataFrame with Bitcoin price data
            sentiment_df: DataFrame with Wikipedia sentiment scores
            
        Returns:
            Dictionary containing processed datasets
        """
        logger.info("Preparing data for ML model...")
        
        # Ensure date columns are datetime
        price_df['date'] = pd.to_datetime(price_df['date'])
        sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])
        
        # Create technical indicators
        price_df = self._create_technical_features(price_df)
        
        # Merge price and sentiment data
        merged_df = pd.merge(price_df, sentiment_df[['date', 'final_compound']], 
                           on='date', how='left')
        
        # Forward fill missing sentiment data
        merged_df['final_compound'] = merged_df['final_compound'].fillna(method='ffill').fillna(0)
        
        # Sort by date
        merged_df = merged_df.sort_values('date').reset_index(drop=True)
        
        # Prepare features
        price_features = merged_df[self.feature_columns].values
        sentiment_features = merged_df['final_compound'].values
        targets = merged_df['price_usd'].values
        
        # Scale features
        price_features_scaled = self.price_scaler.fit_transform(price_features)
        sentiment_features_scaled = self.sentiment_scaler.fit_transform(
            sentiment_features.reshape(-1, 1)
        ).flatten()
        targets_scaled = self.target_scaler.fit_transform(targets.reshape(-1, 1)).flatten()
        
        # Create train/validation/test splits (time series aware)
        total_samples = len(merged_df)
        train_size = int(0.7 * total_samples)
        val_size = int(0.15 * total_samples)
        
        train_data = {
            'price_features': price_features_scaled[:train_size],
            'sentiment_features': sentiment_features_scaled[:train_size],
            'targets': targets_scaled[:train_size],
            'dates': merged_df['date'][:train_size]
        }
        
        val_data = {
            'price_features': price_features_scaled[train_size:train_size + val_size],
            'sentiment_features': sentiment_features_scaled[train_size:train_size + val_size],
            'targets': targets_scaled[train_size:train_size + val_size],
            'dates': merged_df['date'][train_size:train_size + val_size]
        }
        
        test_data = {
            'price_features': price_features_scaled[train_size + val_size:],
            'sentiment_features': sentiment_features_scaled[train_size + val_size:],
            'targets': targets_scaled[train_size + val_size:],
            'dates': merged_df['date'][train_size + val_size:]
        }
        
        logger.info(f"Data preparation complete:")
        logger.info(f"  Training samples: {len(train_data['targets'])}")
        logger.info(f"  Validation samples: {len(val_data['targets'])}")
        logger.info(f"  Test samples: {len(test_data['targets'])}")
        logger.info(f"  Features: {len(self.feature_columns)} price + 1 sentiment")
        
        return {
            'train': train_data,
            'validation': val_data,
            'test': test_data,
            'merged_df': merged_df,
            'feature_columns': self.feature_columns
        }
    
    def _create_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create technical analysis features for price prediction."""
        df = df.copy()
        
        # Volatility features
        df['volatility_7d'] = df['price_usd'].rolling(window=7).std()
        df['volatility_30d'] = df['price_usd'].rolling(window=30).std()
        
        # Moving averages
        df['sma_7'] = df['price_usd'].rolling(window=7).mean()
        df['sma_30'] = df['price_usd'].rolling(window=30).mean()
        
        # RSI (Relative Strength Index)
        delta = df['price_usd'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi_14'] = 100 - (100 / (1 + rs))
        
        # Fill NaN values
        df = df.fillna(method='bfill').fillna(0)
        
        return df
    
    def build_model(self, input_size: int) -> nn.Module:
        """Build the specified model architecture."""
        if self.model_type == 'lstm':
            model = BitcoinLSTMModel(
                input_size=input_size + 1,  # +1 for sentiment
                hidden_size=128,
                num_layers=2,
                dropout=0.2
            )
        elif self.model_type == 'transformer':
            model = BitcoinTransformerModel(
                input_size=input_size + 1,  # +1 for sentiment
                d_model=128,
                nhead=8,
                num_layers=4,
                dropout=0.1
            )
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
        
        return model.to(self.device)
    
    def train_model(self, data: Dict, epochs: int = 100, batch_size: int = 32, 
                   learning_rate: float = 0.001) -> Dict:
        """
        Train the Bitcoin price prediction model.
        
        Returns:
            Dictionary containing training metrics and history
        """
        logger.info(f"Training {self.model_type} model...")
        
        # Create datasets
        train_dataset = BitcoinPredictionDataset(
            data['train']['price_features'],
            data['train']['sentiment_features'],
            data['train']['targets'],
            self.sequence_length
        )
        
        val_dataset = BitcoinPredictionDataset(
            data['validation']['price_features'],
            data['validation']['sentiment_features'],
            data['validation']['targets'],
            self.sequence_length
        )
        
        # Create data loaders
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # Build model
        input_size = len(self.feature_columns)
        self.model = self.build_model(input_size)
        
        # Define loss function and optimizer
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
        
        # Training history
        history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': []
        }
        
        best_val_loss = float('inf')
        patience_counter = 0
        early_stopping_patience = 20
        
        for epoch in range(epochs):
            # Training phase
            self.model.train()
            train_losses = []
            
            for batch_features, batch_targets in train_loader:
                batch_features = batch_features.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                optimizer.zero_grad()
                
                predictions, uncertainty, _ = self.model(batch_features)
                loss = criterion(predictions.squeeze(), batch_targets)
                
                # Add uncertainty regularization (encourage calibrated uncertainty)
                uncertainty_loss = torch.mean(uncertainty)
                total_loss = loss + 0.01 * uncertainty_loss
                
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_losses.append(loss.item())
            
            # Validation phase
            self.model.eval()
            val_losses = []
            
            with torch.no_grad():
                for batch_features, batch_targets in val_loader:
                    batch_features = batch_features.to(self.device)
                    batch_targets = batch_targets.to(self.device)
                    
                    predictions, _, _ = self.model(batch_features)
                    loss = criterion(predictions.squeeze(), batch_targets)
                    val_losses.append(loss.item())
            
            # Calculate average losses
            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)
            
            # Update learning rate
            scheduler.step(avg_val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            # Save history
            history['train_loss'].append(avg_train_loss)
            history['val_loss'].append(avg_val_loss)
            history['learning_rate'].append(current_lr)
            
            # Early stopping
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), f'best_bitcoin_{self.model_type}_model.pth')
            else:
                patience_counter += 1
            
            # Logging
            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}/{epochs}: "
                          f"Train Loss: {avg_train_loss:.6f}, "
                          f"Val Loss: {avg_val_loss:.6f}, "
                          f"LR: {current_lr:.6f}")
            
            # Early stopping
            if patience_counter >= early_stopping_patience:
                logger.info(f"Early stopping at epoch {epoch}")
                break
        
        # Load best model
        self.model.load_state_dict(torch.load(f'best_bitcoin_{self.model_type}_model.pth'))
        
        logger.info(f"Training completed. Best validation loss: {best_val_loss:.6f}")
        
        return history
    
    def evaluate_model(self, data: Dict) -> Dict:
        """
        Evaluate the trained model on test data.
        
        Returns:
            Dictionary containing evaluation metrics and predictions
        """
        logger.info("Evaluating model on test data...")
        
        test_dataset = BitcoinPredictionDataset(
            data['test']['price_features'],
            data['test']['sentiment_features'],
            data['test']['targets'],
            self.sequence_length
        )
        
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
        
        self.model.eval()
        predictions = []
        uncertainties = []
        actuals = []
        
        with torch.no_grad():
            for batch_features, batch_targets in test_loader:
                batch_features = batch_features.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                pred, unc, _ = self.model(batch_features)
                
                predictions.extend(pred.cpu().numpy().flatten())
                uncertainties.extend(unc.cpu().numpy().flatten())
                actuals.extend(batch_targets.cpu().numpy())
        
        # Convert back to original scale
        predictions = self.target_scaler.inverse_transform(
            np.array(predictions).reshape(-1, 1)
        ).flatten()
        actuals = self.target_scaler.inverse_transform(
            np.array(actuals).reshape(-1, 1)
        ).flatten()
        
        # Calculate metrics
        mse = mean_squared_error(actuals, predictions)
        mae = mean_absolute_error(actuals, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(actuals, predictions)
        
        # Mean Absolute Percentage Error
        mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100
        
        # Directional accuracy (did we predict the right direction?)
        actual_direction = np.diff(actuals) > 0
        pred_direction = np.diff(predictions) > 0
        directional_accuracy = np.mean(actual_direction == pred_direction) * 100
        
        metrics = {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'r2_score': r2,
            'mape': mape,
            'directional_accuracy': directional_accuracy,
            'predictions': predictions,
            'actuals': actuals,
            'uncertainties': uncertainties,
            'test_dates': data['test']['dates'].iloc[self.sequence_length:].values
        }
        
        logger.info(f"Model Evaluation Results:")
        logger.info(f"  RMSE: ${rmse:.2f}")
        logger.info(f"  MAE: ${mae:.2f}")
        logger.info(f"  MAPE: {mape:.2f}%")
        logger.info(f"  R¬≤: {r2:.4f}")
        logger.info(f"  Directional Accuracy: {directional_accuracy:.2f}%")
        
        return metrics
    
    def predict_next_price(self, recent_price_data: np.ndarray, 
                          recent_sentiment: float) -> Tuple[float, float]:
        """
        Predict the next Bitcoin price with uncertainty.
        
        Args:
            recent_price_data: Last 30 days of price features
            recent_sentiment: Recent sentiment score
            
        Returns:
            Tuple of (predicted_price, uncertainty)
        """
        self.model.eval()
        
        # Prepare input data
        price_scaled = self.price_scaler.transform(recent_price_data)
        sentiment_scaled = self.sentiment_scaler.transform([[recent_sentiment]]).flatten()
        
        # Create input tensor
        combined_features = np.concatenate([
            price_scaled, 
            sentiment_scaled.reshape(-1, 1)
        ], axis=1)
        
        input_tensor = torch.FloatTensor(combined_features).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            prediction, uncertainty, _ = self.model(input_tensor)
            
            # Convert back to original scale
            pred_scaled = prediction.cpu().numpy()
            pred_price = self.target_scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]
            
            uncertainty_value = uncertainty.cpu().numpy()[0, 0]
            
        return pred_price, uncertainty_value
    
    def create_baseline_models(self, data: Dict) -> Dict:
        """
        Create baseline models (Random Forest, XGBoost) for comparison.
        """
        logger.info("Training baseline models...")
        
        # Prepare data for traditional ML
        def prepare_traditional_ml_data(price_features, sentiment_features, targets):
            # Create sequences manually for traditional ML
            sequences = []
            sequence_targets = []
            
            for i in range(len(price_features) - self.sequence_length):
                # Flatten the sequence
                price_seq = price_features[i:i + self.sequence_length].flatten()
                sentiment_seq = sentiment_features[i:i + self.sequence_length]
                
                # Combine features
                combined = np.concatenate([price_seq, sentiment_seq])
                sequences.append(combined)
                sequence_targets.append(targets[i + self.sequence_length])
            
            return np.array(sequences), np.array(sequence_targets)
        
        # Prepare training data
        X_train, y_train = prepare_traditional_ml_data(
            data['train']['price_features'],
            data['train']['sentiment_features'],
            data['train']['targets']
        )
        
        X_test, y_test = prepare_traditional_ml_data(
            data['test']['price_features'],
            data['test']['sentiment_features'],
            data['test']['targets']
        )
        
        baseline_results = {}
        
        # Random Forest
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        rf_model.fit(X_train, y_train)
        rf_predictions = rf_model.predict(X_test)
        
        # Convert back to original scale
        rf_predictions_original = self.target_scaler.inverse_transform(
            rf_predictions.reshape(-1, 1)
        ).flatten()
        y_test_original = self.target_scaler.inverse_transform(
            y_test.reshape(-1, 1)
        ).flatten()
        
        baseline_results['random_forest'] = {
            'rmse': np.sqrt(mean_squared_error(y_test_original, rf_predictions_original)),
            'mae': mean_absolute_error(y_test_original, rf_predictions_original),
            'r2': r2_score(y_test_original, rf_predictions_original),
            'predictions': rf_predictions_original
        }
        
        # XGBoost
        xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        xgb_model.fit(X_train, y_train)
        xgb_predictions = xgb_model.predict(X_test)
        
        xgb_predictions_original = self.target_scaler.inverse_transform(
            xgb_predictions.reshape(-1, 1)
        ).flatten()
        
        baseline_results['xgboost'] = {
            'rmse': np.sqrt(mean_squared_error(y_test_original, xgb_predictions_original)),
            'mae': mean_absolute_error(y_test_original, xgb_predictions_original),
            'r2': r2_score(y_test_original, xgb_predictions_original),
            'predictions': xgb_predictions_original
        }
        
        logger.info("Baseline models trained:")
        for model_name, results in baseline_results.items():
            logger.info(f"  {model_name}: RMSE=${results['rmse']:.2f}, R¬≤={results['r2']:.4f}")
        
        return baseline_results
    
    def save_model(self, filepath: str):
        """Save the trained model and scalers."""
        model_data = {
            'model_state_dict': self.model.state_dict(),
            'model_type': self.model_type,
            'price_scaler': self.price_scaler,
            'sentiment_scaler': self.sentiment_scaler,
            'target_scaler': self.target_scaler,
            'feature_columns': self.feature_columns,
            'sequence_length': self.sequence_length
        }
        
        torch.save(model_data, filepath)
        logger.info(f"Model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """Load a trained model and scalers."""
        model_data = torch.load(filepath, map_location=self.device)
        
        self.model_type = model_data['model_type']
        self.price_scaler = model_data['price_scaler']
        self.sentiment_scaler = model_data['sentiment_scaler']
        self.target_scaler = model_data['target_scaler']
        self.feature_columns = model_data['feature_columns']
        self.sequence_length = model_data['sequence_length']
        
        # Rebuild model
        input_size = len(self.feature_columns)
        self.model = self.build_model(input_size)
        self.model.load_state_dict(model_data['model_state_dict'])
        
        logger.info(f"Model loaded from {filepath}")

# Visualization and analysis functions
def visualize_training_history(history: Dict):
    """Visualize model training history."""
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # Loss curves
    axes[0].plot(history['train_loss'], label='Training Loss', alpha=0.8)
    axes[0].plot(history['val_loss'], label='Validation Loss', alpha=0.8)
    axes[0].set_title('Model Loss During Training')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Learning rate
    axes[1].plot(history['learning_rate'], label='Learning Rate', color='orange')
    axes[1].set_title('Learning Rate Schedule')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Learning Rate')
    axes[1].set_yscale('log')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def visualize_predictions(metrics: Dict):
    """Visualize model predictions vs actual prices."""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Time series plot
    axes[0, 0].plot(metrics['test_dates'], metrics['actuals'], 
                   label='Actual Price', alpha=0.8, linewidth=2)
    axes[0, 0].plot(metrics['test_dates'], metrics['predictions'], 
                   label='Predicted Price', alpha=0.8, linewidth=2)
    axes[0, 0].set_title('Bitcoin Price Prediction vs Actual')
    axes[0, 0].set_xlabel('Date')
    axes[0, 0].set_ylabel('Price (USD)')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # Scatter plot
    axes[0, 1].scatter(metrics['actuals'], metrics['predictions'], alpha=0.6)
    axes[0, 1].plot([metrics['actuals'].min(), metrics['actuals'].max()], 
                   [metrics['actuals'].min(), metrics['actuals'].max()], 
                   'r--', lw=2)
    axes[0, 1].set_title('Predicted vs Actual Prices')
    axes[0, 1].set_xlabel('Actual Price (USD)')
    axes[0, 1].set_ylabel('Predicted Price (USD)')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Prediction errors
    errors = metrics['predictions'] - metrics['actuals']
    axes[1, 0].hist(errors, bins=30, alpha=0.7, edgecolor='black')
    axes[1, 0].set_title('Prediction Error Distribution')
    axes[1, 0].set_xlabel('Error (USD)')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Uncertainty visualization (if available)
    if 'uncertainties' in metrics:
        axes[1, 1].scatter(metrics['predictions'], metrics['uncertainties'], alpha=0.6)
        axes[1, 1].set_title('Prediction Uncertainty')
        axes[1, 1].set_xlabel('Predicted Price (USD)')
        axes[1, 1].set_ylabel('Uncertainty')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# Example usage and testing
if __name__ == "__main__":
    print("\n" + "="*60)
    print("BITCOIN PRICE PREDICTION ML SYSTEM")
    print("="*60)
    
    # Initialize ML system
    ml_system = BitcoinMLSystem(model_type='lstm')
    
    print(f"\nü§ñ ML System Initialized:")
    print(f"   Model Type: LSTM with Attention")
    print(f"   Device: {ml_system.device}")
    print(f"   Features: Price data + Wikipedia sentiment")
    print(f"   Sequence Length: {ml_system.sequence_length} days")
    
    print(f"\nüìã Next Steps for Full Implementation:")
    print(f"   1. Load Bitcoin price data (from Step 2)")
    print(f"   2. Load Wikipedia sentiment data (from Step 1)")
    print(f"   3. Run: prepared_data = ml_system.prepare_data(price_df, sentiment_df)")
    print(f"   4. Run: history = ml_system.train_model(prepared_data)")
    print(f"   5. Run: metrics = ml_system.evaluate_model(prepared_data)")
    print(f"   6. Visualize results and deploy for dashboard")
    
    print(f"\nüè¶ Banking-Grade Features:")
    print(f"   ‚úÖ Uncertainty quantification for risk management")
    print(f"   ‚úÖ Multiple model architectures (LSTM, Transformer)")
    print(f"   ‚úÖ Baseline model comparison (Random Forest, XGBoost)")
    print(f"   ‚úÖ Professional validation and backtesting")
    print(f"   ‚úÖ Real-time prediction capabilities")
    print(f"   ‚úÖ Model interpretability and explainability")
    
    print(f"\n" + "="*60)
    print("üöÄ READY FOR STEP 4: BACKTESTING & VALIDATION")
    print("="*60)
